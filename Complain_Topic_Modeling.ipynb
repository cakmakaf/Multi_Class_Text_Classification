{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "\n",
    "In this notebook, I will try to approach to the problem as a topic modeling, specificall Latent Dirichlet Allocation (LDA) problem that has strong statistical influence on NLP text classification probems. The main objective of topic modeling is to find different type of topics that are exist in our row data (text column in th case_study_data.csv file).\n",
    "\n",
    "Each document in the raw data will be generated of at least one or multiple topics. Once the technique is applied, our job as a human is to interpret the outputs and detect if the mixture of words in each topic make sense. If they don't make sense, we could tune the number of topics, the terms in the document-term matrix, model parameters, or even try a different model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling & Latent Dirichlet Allocation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract \"topics\" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body.\n",
    "\n",
    "The LDA is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. It is used to classify text in a document to a certai topic. It builds a topic per document model and words per topic model, modeled as Dirichlet distributions.  \n",
    "\n",
    "* Each document is modeled as a multinomial distribution of topics and each topic is modeled as a multinomial distribution of words.\n",
    "* LDA assumes that the every chunk of text we feed into it will contain words that are somehow related. Consequently,  selecting the correct corpus of data is crucial. \n",
    "* It also postulates documents are made up from a mixture of topics. Those topics then produce words depend on their probability distributions. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Load the dataset\n",
    "\n",
    "The dataset we'll use is a list of opened complain cases with 268,363 records that contains complaint text (**text**), a message identifier (**complaint_id**) and a verified correct complaint department (**product_group**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Read the dataset as CSV file and save the text part to 'data_text'\n",
    "\n",
    "import pandas as pd\n",
    "data = pd.read_csv('case_study_data.csv', error_bad_lines=False);\n",
    "# We only need the 'text' column from the raw data as a corpus(document)\n",
    "data_text = data[:268363][['text']];\n",
    "data_text['index'] = data_text.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>complaint_id</th>\n",
       "      <th>product_group</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2815595</td>\n",
       "      <td>bank_service</td>\n",
       "      <td>On XX/XX/2017 my check # XXXX was debited from...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2217937</td>\n",
       "      <td>bank_service</td>\n",
       "      <td>I opened a Bank of the the West account. The a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2657456</td>\n",
       "      <td>bank_service</td>\n",
       "      <td>wells fargo in nj opened a business account wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1414106</td>\n",
       "      <td>bank_service</td>\n",
       "      <td>A hold was placed on my saving account ( XXXX ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1999158</td>\n",
       "      <td>bank_service</td>\n",
       "      <td>Dear CFPB : I need to send a major concern/com...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   complaint_id product_group  \\\n",
       "0       2815595  bank_service   \n",
       "1       2217937  bank_service   \n",
       "2       2657456  bank_service   \n",
       "3       1414106  bank_service   \n",
       "4       1999158  bank_service   \n",
       "\n",
       "                                                text  \n",
       "0  On XX/XX/2017 my check # XXXX was debited from...  \n",
       "1  I opened a Bank of the the West account. The a...  \n",
       "2  wells fargo in nj opened a business account wi...  \n",
       "3  A hold was placed on my saving account ( XXXX ...  \n",
       "4  Dear CFPB : I need to send a major concern/com...  "
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First glimpse to the data_text\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bank_service', 'credit_card', 'credit_reporting',\n",
       "       'debt_collection', 'loan', 'money_transfers', 'mortgage'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all distinct `product_group`\n",
    "dist_product_group = data.product_group.unique()\n",
    "dist_product_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dist_product_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>On XX/XX/2017 my check # XXXX was debited from...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I opened a Bank of the the West account. The a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>wells fargo in nj opened a business account wi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A hold was placed on my saving account ( XXXX ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear CFPB : I need to send a major concern/com...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  index\n",
       "0  On XX/XX/2017 my check # XXXX was debited from...      0\n",
       "1  I opened a Bank of the the West account. The a...      1\n",
       "2  wells fargo in nj opened a business account wi...      2\n",
       "3  A hold was placed on my saving account ( XXXX ...      3\n",
       "4  Dear CFPB : I need to send a major concern/com...      4"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First glimpse to the data_text\n",
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "268363\n"
     ]
    }
   ],
   "source": [
    "# Check the rows of the dataset\n",
    "print(len(data_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(268363, 2)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the data_text \n",
    "data_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2- Data Preprocessing \n",
    "\n",
    "For data preprocessing we will execute the following steps\n",
    "\n",
    "* **Tokenization**: We will split the data into sentences and the sentences into words, then lowercase the entire data_text and remove all punctuations.\n",
    "* **Remove < three letters**: All the words that have less than **three letters** will be removed. \n",
    "* **Remove Stopwords**: All the **stopwords** are removed.\n",
    "* **Lemmatization**: The words concerning third party will be changed to first person and all past/future tense verbs are converted to present tense  \n",
    "* **stemming**: All words will be reduced to their root form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ahmetcakmak/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import gensim and nltk libraries\n",
    "\n",
    "# pip install gensim\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean all the 'text' column\n",
    "data_text.text = data_text.text.str.replace(r'[^a-zA-Z\\s]+|X{2,}', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>On  my check   was debited from my checking ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I opened a Bank of the the West account The ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>wells fargo in nj opened a business account wi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A hold was placed on my saving account    beca...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear CFPB  I need to send a major concerncompl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  index\n",
       "0  On  my check   was debited from my checking ac...      0\n",
       "1  I opened a Bank of the the West account The ac...      1\n",
       "2  wells fargo in nj opened a business account wi...      2\n",
       "3  A hold was placed on my saving account    beca...      3\n",
       "4  Dear CFPB  I need to send a major concerncompl...      4"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - 1:\n",
    "\n",
    "We will give a short example of lemmatization using before preprocessing our entire data_text. Let's see what output would be obtained if we lemmatize the word 'given'?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give\n"
     ]
    }
   ],
   "source": [
    "# Convert past tense to present tense\n",
    "print(WordNetLemmatizer().lemmatize('given', pos = 'v')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example - 2:\n",
    "In this example, we will give gow a stemming works? I will pick a number of words from the dataset and see how the stemmer deal with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original word</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>account</td>\n",
       "      <td>account</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>save</td>\n",
       "      <td>save</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>deducted</td>\n",
       "      <td>deduct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>deduction</td>\n",
       "      <td>deduct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>transaction</td>\n",
       "      <td>transact</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>transferred</td>\n",
       "      <td>transfer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>landlord</td>\n",
       "      <td>landlord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>agreed</td>\n",
       "      <td>agre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>owned</td>\n",
       "      <td>own</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>fee</td>\n",
       "      <td>fee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>mortgagemeeting</td>\n",
       "      <td>mortgagemeet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      original word       stemmed\n",
       "0           account       account\n",
       "1              save          save\n",
       "2          deducted        deduct\n",
       "3         deduction        deduct\n",
       "4       transaction      transact\n",
       "5       transferred      transfer\n",
       "6          landlord      landlord\n",
       "7            agreed          agre\n",
       "8             owned           own\n",
       "9               fee           fee\n",
       "10  mortgagemeeting  mortgagemeet"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "original_words = ['account','save','deducted','deduction','transaction','transferred','landlord','agreed','owned', \n",
    "           'fee','mortgage''meeting']\n",
    "singles = [stemmer.stem(plural) for plural in original_words]\n",
    "\n",
    "pd.DataFrame(data={'original word':original_words, 'stemmed':singles })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the following function executes the pre-processing step for our entire dataset \n",
    "#pre-processing steps on the data_text\n",
    "\n",
    "def lemm_stemm(x_text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(x_text, pos='v'))\n",
    "\n",
    "# tokenize and lemmatize\n",
    "def preprocess(x_text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(x_text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            # perform lemm_stemm on the token, then append to the result lis \n",
    "            result.append(lemm_stemm(token))\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original raw document: \n",
      "['they', 'closed', 'my', 'account', 'because', 'im', '', 'which', 'a', 'federal', 'lawsuit', 'is', 'being', 'prepared', 'and', 'about', 'to', 'be', 'filed', 'against', 'them', 'but', 'they', 'have', 'no', 'sent', 'me', 'my', 'money', 'there', 'holding', 'my', 'money', 'and', 'wont', 'tell', 'me', 'when', 'there', 'sending', 'my', 'money']\n",
      "\n",
      "\n",
      "tokenized and lemmatized document: \n",
      "['close', 'account', 'feder', 'lawsuit', 'prepar', 'file', 'send', 'money', 'hold', 'money', 'wont', 'tell', 'send', 'money']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Preview a document after preprocessing\n",
    "\n",
    "document_num = 146\n",
    "sample_document = documents[documents['index'] == document_num].values[0][0]\n",
    "\n",
    "print(\"original raw document: \")\n",
    "words = []\n",
    "for word in sample_document.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\ntokenized and lemmatized document: \")\n",
    "print(preprocess(sample_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>On  my check   was debited from my checking ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>I opened a Bank of the the West account The ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>wells fargo in nj opened a business account wi...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>A hold was placed on my saving account    beca...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Dear CFPB  I need to send a major concerncompl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268358</td>\n",
       "      <td>I have recently been declined for a loan due t...</td>\n",
       "      <td>268358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268359</td>\n",
       "      <td>I am   military I requested help from Loan car...</td>\n",
       "      <td>268359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268360</td>\n",
       "      <td>The collections department at Wells Fargo bega...</td>\n",
       "      <td>268360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268361</td>\n",
       "      <td>I was denied the chance to continue an applica...</td>\n",
       "      <td>268361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268362</td>\n",
       "      <td>On  my husband and I inquired about a home mor...</td>\n",
       "      <td>268362</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>268363 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text   index\n",
       "0       On  my check   was debited from my checking ac...       0\n",
       "1       I opened a Bank of the the West account The ac...       1\n",
       "2       wells fargo in nj opened a business account wi...       2\n",
       "3       A hold was placed on my saving account    beca...       3\n",
       "4       Dear CFPB  I need to send a major concerncompl...       4\n",
       "...                                                   ...     ...\n",
       "268358  I have recently been declined for a loan due t...  268358\n",
       "268359  I am   military I requested help from Loan car...  268359\n",
       "268360  The collections department at Wells Fargo bega...  268360\n",
       "268361  I was denied the chance to continue an applica...  268361\n",
       "268362  On  my husband and I inquired about a home mor...  268362\n",
       "\n",
       "[268363 rows x 2 columns]"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: \n",
    "For the sake of avoiding a further confusion, I assign the **data_text** column as the **documents** of the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call each row of data_text a document, therefore documents for data_text\n",
    "documents = data_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to preprocess all the complains of WF we have in data_text. To make it happen, let's use the [map function](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html) from pandas to apply `preprocess()` to the `text` column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess all the complains, saving the list of results as 'processed_docs'\n",
    "processed_data = documents['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [check, debit, check, account, check, wasnt, c...\n",
       "1    [open, bank, west, account, account, come, pro...\n",
       "2    [well, fargo, open, busi, account, author, ear...\n",
       "3    [hold, place, save, account, institut, say, ma...\n",
       "4    [dear, cfpb, need, send, major, fals, advertis...\n",
       "5    [bank, america, close, check, account, prior, ...\n",
       "6    [complaint, bank, america, account, start, loc...\n",
       "7    [open, account, saturday, well, fargo, go, com...\n",
       "8    [busi, partner, experi, total, breakdown, comm...\n",
       "9    [want, credit, increas, credit, card, chase, b...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view the first 10 'processed_data'\n",
    "processed_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Bag of Words (BoW) on the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words (BoW)   ###\n",
    "\n",
    "Now, we will create a dictionary from '**processed_data**' involved the number of times a word appears in the training set. In order to be able to apply that, we will transmit `processed_data` to [`gensim.corpora.Dictionary()`](https://radimrehurek.com/gensim/corpora/dictionary.html) and call it as '`BoW_dictionary`'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using gensim.corpora.Dictionary, create a dictionary from the preprocess data ('processed_data') \n",
    "# involves the number of times a word appears in the training set, and call it 'dictionary'\n",
    "BoW_dictionary = gensim.corpora.Dictionary(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84129"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cheking the size of BoW_dictionary\n",
    "len(BoW_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 account\n",
      "1 america\n",
      "2 answer\n",
      "3 bank\n",
      "4 breach\n",
      "5 cash\n",
      "6 check\n",
      "7 contract\n",
      "8 contractor\n",
      "9 copi\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 items in the BoW_dictionary we created.\n",
    "count = 0\n",
    "for k, v in BoW_dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 9:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Gensim, filter_extremes:**\n",
    "\n",
    "This function in the [`Gensim`](https://radimrehurek.com/gensim/index.html) removes all tokens in the dictionary that are:\n",
    "\n",
    "Filter out tokens in the dictionary by their frequency \n",
    "\n",
    "* Less frequent than no_below documents (absolute number, e.g. 5) or\n",
    "\n",
    "* More frequent than no_above documents (fraction of the total corpus size, e.g. 0.3).\n",
    "\n",
    "* After the first two steps above, keep only the first keep_n most frequent tokens (or keep all if keep_n=None).\n",
    "\n",
    "After the pruning, resulting gaps in word ids are shrunk. Due to this gap shrinking, the same word may have a different word id before and after the call to this function!\n",
    "\n",
    "[`filter_extremes(no_below=5, no_above=0.5, keep_n=300000)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove words appearing less than 15 times and words appearing in more than 10% of all documents\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.1, keep_n=300000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From Gensim, doc2bow**\n",
    "\n",
    "[`doc2bow(document)`](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow)\n",
    "\n",
    "It allows us to convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
    "\n",
    "* list of (int, int) – BoW representation of document.\n",
    "\n",
    "* list of (int, int), dict of (str, int) – If return_missing is True, return BoW representation of document + dictionary with missing tokens and their frequencies.\n",
    "\n",
    "* Convert document (a list of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Bag of words model for each document.\n",
    "#For each document we create a dictionary, notifying how many words and how many times those words appear. \n",
    "#Then save it to'BoW_data'\n",
    "\n",
    "BoW_data = [BoW_dictionary.doc2bow(document) for document in processed_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (58, 1),\n",
       " (64, 1),\n",
       " (72, 3),\n",
       " (86, 1),\n",
       " (174, 2),\n",
       " (217, 1),\n",
       " (715, 1),\n",
       " (746, 1),\n",
       " (1321, 1),\n",
       " (1345, 1)]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check BoW_data for our sample document (document_num = 146) assigning as (token_id, token_count)\n",
    "\n",
    "BoW_data[document_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word 0 (\"account\") appears 1 time.\n",
      "The word 58 (\"close\") appears 1 time.\n",
      "The word 64 (\"hold\") appears 1 time.\n",
      "The word 72 (\"money\") appears 3 time.\n",
      "The word 86 (\"tell\") appears 1 time.\n",
      "The word 174 (\"send\") appears 2 time.\n",
      "The word 217 (\"file\") appears 1 time.\n",
      "The word 715 (\"prepar\") appears 1 time.\n",
      "The word 746 (\"feder\") appears 1 time.\n",
      "The word 1321 (\"lawsuit\") appears 1 time.\n",
      "The word 1345 (\"wont\") appears 1 time.\n"
     ]
    }
   ],
   "source": [
    "# View BoW for our sample preprocessed document\n",
    "# Here document_num is document number 4310 which we have checked in Step 2\n",
    "BoW_doc_146 = BoW_data[document_num]\n",
    "\n",
    "for i in range(len(BoW_doc_146)):\n",
    "    print(\"The word {} (\\\"{}\\\") appears {} time.\".format(BoW_doc_146[i][0],\n",
    "                                                     BoW_dictionary[BoW_doc_146[i][0]],\n",
    "                                                     BoW_doc_146[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF ###\n",
    "\n",
    "Although TF-IDF is not compulsory for LDA \n",
    "\n",
    "While performing TF-IDF on the corpus is not necessary for LDA execution using the gensim model, it is highly recemmended. TF-IDF expects a bag-of-words (integer values) training the raw data during initialization. It will take a vector and return another vector of the same dimensionality right along the transformation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** TF-IDF, short for \"Term Frequency, Inverse Document Frequency\".**\n",
    "\n",
    "* It is a way to score the importance of words (or \"terms\") in a document based on how frequently they appear across multiple documents.\n",
    "* If a word appears frequently in a document, it's important. Give the word a high score. But if a word appears in many documents, it's not a unique identifier. Give the word a low score.\n",
    "* Therefore, common words like \"the\" and \"for\", which appear in many documents, will be scaled down. Words that appear frequently in a single document will be scaled up.\n",
    "\n",
    "In other words:\n",
    "\n",
    "* TF(w) = `(Number of times term w appears in a document) / (Total number of terms in the document)`.\n",
    "* IDF(w) = `log_e(Total number of documents / Number of documents with term w in it)`.\n",
    "\n",
    "** For example **\n",
    "\n",
    "* Consider a document containing `100` words wherein the word 'tiger' appears 3 times. \n",
    "* The term frequency (i.e., tf) for 'tiger' is then: \n",
    "    - `TF = (3 / 100) = 0.03`. \n",
    "\n",
    "* Now, assume we have `10 million` documents and the word 'tiger' appears in `1000` of these. Then, the inverse document frequency (i.e., idf) is calculated as:\n",
    "    - `IDF = log(10,000,000 / 1,000) = 4`. \n",
    "\n",
    "* Thus, the Tf-idf weight is the product of these quantities: \n",
    "    - `TF-IDF = 0.03 * 4 = 0.12`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tf-idf model, using models.TfidfModel on 'BoW_data' and save it to 'tfidf'\n",
    "\n",
    "from gensim import corpora, models\n",
    "\n",
    "tfidf = models.TfidfModel(BoW_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply mapping to the entire raw data and call it 'tfidf_data'\n",
    "\n",
    "tfidf_data = tfidf[BoW_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.045825291607662355),\n",
      " (1, 0.2120719192479975),\n",
      " (2, 0.1723888484344525),\n",
      " (3, 0.09613578316476834),\n",
      " (4, 0.24305337933065752),\n",
      " (5, 0.20752190679544874),\n",
      " (6, 0.3474928245414018),\n",
      " (7, 0.18097198195724792),\n",
      " (8, 0.358197632746281),\n",
      " (9, 0.14544491750720015),\n",
      " (10, 0.17889193043554064),\n",
      " (11, 0.03788818381484691),\n",
      " (12, 0.21607305152235232),\n",
      " (13, 0.3593746431907906),\n",
      " (14, 0.1679352428545851),\n",
      " (15, 0.10406549293031389),\n",
      " (16, 0.07307272424878672),\n",
      " (17, 0.15467907071791237),\n",
      " (18, 0.09226974821905376),\n",
      " (19, 0.37461015664244207),\n",
      " (20, 0.28713105349636575)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# View TF-IDF scores for the first document assigned as (token_id, tfidf score)\n",
    "\n",
    "from pprint import pprint\n",
    "for doc in tfidf_data:\n",
    "    pprint(doc)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4- Executing LDA by use of BoW ##\n",
    "\n",
    "We are going for 10 topics in the document corpus.\n",
    "\n",
    "** We will be executing LDA using all CPU cores to parallelize and speed up model training.**\n",
    "\n",
    "Some of the parameters we will pull are:\n",
    "\n",
    "* **n_topics: int, optional (default=10)** is the number of requested hidden (latent) topics to be extracted from the training raw data.\n",
    "* **id2word:** is a transformation from *word_id* (integers) to *word* (strings). It allows us to specify the vocabulary size, along with for debugging and topic printing.\n",
    "* **workers:** is the number of extra processes to use for parallelization. Uses all available cores by default.\n",
    "* **alpha, $\\alpha$** and **eta, $\\eta$:** are hyperparameters that affect sparsity of the document-topic (theta, $\\theta$) and topic-word (lambda, $\\lambda$) distributions. We will let these be the default values for now(default value is $\\frac{1}{n\\_topics}$)\n",
    "    - $\\alpha$ is the per document topic distribution.\n",
    "        * High **alpha**: Every document has a mixture of all topics(documents appear similar to each other).\n",
    "        * Low **alpha**: Every document has a mixture of very few topics\n",
    "\n",
    "    - $\\eta$ is the per topic word distribution.\n",
    "        * High **eta**: Each topic has a mixture of most words(topics appear similar to each other).\n",
    "        * Low **eta**: Each topic has a mixture of few words.\n",
    "\n",
    "* **passes:** is the number of training passes through the corpus. For  example, if the training corpus has 50,000 documents, chunksize is  10,000, passes is 2, then online training is done in 10 updates: \n",
    "    * `#1 documents 0-9,999 `\n",
    "    * `#2 documents 10,000-19,999 `\n",
    "    * `#3 documents 20,000-29,999 `\n",
    "    * `#4 documents 30,000-39,999 `\n",
    "    * `#5 documents 40,000-49,999 `\n",
    "    * `#6 documents 0-9,999 `\n",
    "    * `#7 documents 10,000-19,999 `\n",
    "    * `#8 documents 20,000-29,999 `\n",
    "    * `#9 documents 30,000-39,999 `\n",
    "    * `#10 documents 40,000-49,999` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA mono-core\n",
    "# lda_model = gensim.models.LdaModel(bow_corpus, \n",
    "#                                    num_topics = 10, \n",
    "#                                    id2word = dictionary,                                    \n",
    "#                                    passes = 50)\n",
    "\n",
    "# LDA multicore: Train our lda model by gensim.models.LdaMulticore and save it to 'LDA_model'\n",
    " \n",
    "'''\n",
    "'''\n",
    "\n",
    "LDA_model = gensim.models.LdaMulticore(BoW_data, \n",
    "                                       num_topics=10, \n",
    "                                       id2word = BoW_dictionary, \n",
    "                                       passes = 2, \n",
    "                                       workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic: 0 \n",
      "Words: 0.056*\"chase\" + 0.027*\"fraud\" + 0.024*\"secur\" + 0.020*\"inform\" + 0.017*\"person\" + 0.015*\"address\" + 0.014*\"number\" + 0.013*\"social\" + 0.011*\"fraudul\" + 0.011*\"steal\"\n",
      "\n",
      "\n",
      "The topic: 1 \n",
      "Words: 0.046*\"credit\" + 0.045*\"card\" + 0.026*\"citi\" + 0.022*\"capit\" + 0.016*\"call\" + 0.014*\"phone\" + 0.014*\"applic\" + 0.014*\"number\" + 0.014*\"tell\" + 0.013*\"line\"\n",
      "\n",
      "\n",
      "The topic: 2 \n",
      "Words: 0.110*\"account\" + 0.108*\"bank\" + 0.045*\"check\" + 0.027*\"america\" + 0.026*\"charg\" + 0.025*\"money\" + 0.023*\"close\" + 0.020*\"fund\" + 0.018*\"balanc\" + 0.016*\"pay\"\n",
      "\n",
      "\n",
      "The topic: 3 \n",
      "Words: 0.145*\"credit\" + 0.126*\"report\" + 0.048*\"account\" + 0.032*\"remov\" + 0.018*\"inform\" + 0.017*\"score\" + 0.014*\"compani\" + 0.014*\"disput\" + 0.014*\"bureaus\" + 0.011*\"inquiri\"\n",
      "\n",
      "\n",
      "The topic: 4 \n",
      "Words: 0.072*\"loan\" + 0.050*\"mortgag\" + 0.023*\"home\" + 0.019*\"modif\" + 0.013*\"properti\" + 0.013*\"year\" + 0.011*\"foreclosur\" + 0.011*\"well\" + 0.011*\"fargo\" + 0.010*\"escrow\"\n",
      "\n",
      "\n",
      "The topic: 5 \n",
      "Words: 0.044*\"call\" + 0.040*\"tell\" + 0.026*\"say\" + 0.025*\"send\" + 0.025*\"receiv\" + 0.019*\"ask\" + 0.019*\"phone\" + 0.018*\"time\" + 0.017*\"contact\" + 0.017*\"speak\"\n",
      "\n",
      "\n",
      "The topic: 6 \n",
      "Words: 0.026*\"servic\" + 0.015*\"request\" + 0.014*\"email\" + 0.013*\"receiv\" + 0.013*\"process\" + 0.012*\"provid\" + 0.012*\"custom\" + 0.010*\"issu\" + 0.010*\"submit\" + 0.009*\"review\"\n",
      "\n",
      "\n",
      "The topic: 7 \n",
      "Words: 0.120*\"payment\" + 0.042*\"month\" + 0.024*\"pay\" + 0.020*\"late\" + 0.016*\"time\" + 0.016*\"insur\" + 0.011*\"tell\" + 0.009*\"year\" + 0.008*\"compani\" + 0.008*\"go\"\n",
      "\n",
      "\n",
      "The topic: 8 \n",
      "Words: 0.034*\"request\" + 0.033*\"letter\" + 0.027*\"document\" + 0.027*\"send\" + 0.025*\"inform\" + 0.024*\"provid\" + 0.016*\"file\" + 0.016*\"account\" + 0.015*\"report\" + 0.015*\"receiv\"\n",
      "\n",
      "\n",
      "The topic: 9 \n",
      "Words: 0.052*\"debt\" + 0.033*\"collect\" + 0.015*\"court\" + 0.012*\"compani\" + 0.011*\"attorney\" + 0.011*\"state\" + 0.009*\"agenc\" + 0.009*\"legal\" + 0.008*\"claim\" + 0.007*\"servic\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# For each topic, we find out the words occuring in that particular topic and its relative weight\n",
    "\n",
    "for id_x, topic in LDA_model.print_topics(-1):\n",
    "    print(\"The topic: {} \\nWords: {}\".format(id_x, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics ###\n",
    "\n",
    "Using the words in each topic and their corresponding weights, what categories were you able to infer?\n",
    "\n",
    "* 0: \n",
    "* 1: \n",
    "* 2: \n",
    "* 3: \n",
    "* 4: \n",
    "* 5: \n",
    "* 6: \n",
    "* 7:  \n",
    "* 8: \n",
    "* 9: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4.2 Running LDA using TF-IDF ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define lda model using corpus_tfidf\n",
    "'''\n",
    "\n",
    "LDA_model_tfidf = gensim.models.LdaMulticore(tfidf_data, \n",
    "                                             num_topics=10, \n",
    "                                             id2word = BoW_dictionary, \n",
    "                                             passes = 2, \n",
    "                                             workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The topic: 0 Word: 0.015*\"loan\" + 0.013*\"payment\" + 0.012*\"mortgag\" + 0.007*\"month\" + 0.007*\"modif\" + 0.006*\"home\" + 0.006*\"escrow\" + 0.005*\"pay\" + 0.005*\"tell\" + 0.005*\"ocwen\"\n",
      "\n",
      "\n",
      "The topic: 1 Word: 0.021*\"report\" + 0.014*\"credit\" + 0.013*\"debt\" + 0.012*\"account\" + 0.011*\"remov\" + 0.010*\"disput\" + 0.010*\"collect\" + 0.008*\"inform\" + 0.007*\"valid\" + 0.007*\"agenc\"\n",
      "\n",
      "\n",
      "The topic: 2 Word: 0.011*\"debt\" + 0.007*\"mortgag\" + 0.006*\"collect\" + 0.006*\"foreclosur\" + 0.006*\"properti\" + 0.005*\"document\" + 0.005*\"sale\" + 0.005*\"loan\" + 0.005*\"court\" + 0.005*\"letter\"\n",
      "\n",
      "\n",
      "The topic: 3 Word: 0.033*\"inquiri\" + 0.018*\"paypal\" + 0.015*\"hard\" + 0.012*\"salli\" + 0.012*\"author\" + 0.012*\"credit\" + 0.010*\"pull\" + 0.009*\"citibank\" + 0.009*\"card\" + 0.007*\"report\"\n",
      "\n",
      "\n",
      "The topic: 4 Word: 0.016*\"call\" + 0.011*\"phone\" + 0.010*\"number\" + 0.009*\"debt\" + 0.009*\"forbear\" + 0.008*\"tell\" + 0.006*\"work\" + 0.006*\"say\" + 0.006*\"harass\" + 0.006*\"ask\"\n",
      "\n",
      "\n",
      "The topic: 5 Word: 0.012*\"bank\" + 0.011*\"payment\" + 0.010*\"check\" + 0.009*\"account\" + 0.009*\"money\" + 0.009*\"charg\" + 0.008*\"card\" + 0.007*\"transfer\" + 0.007*\"fund\" + 0.006*\"transact\"\n",
      "\n",
      "\n",
      "The topic: 6 Word: 0.146*\"navient\" + 0.032*\"shortag\" + 0.013*\"late\" + 0.013*\"flagstar\" + 0.010*\"payment\" + 0.008*\"defer\" + 0.007*\"ombudsman\" + 0.006*\"statement\" + 0.006*\"stellar\" + 0.006*\"shock\"\n",
      "\n",
      "\n",
      "The topic: 7 Word: 0.020*\"dealer\" + 0.016*\"convent\" + 0.016*\"misappli\" + 0.014*\"enhanc\" + 0.012*\"roof\" + 0.009*\"tila\" + 0.007*\"copay\" + 0.006*\"walmart\" + 0.006*\"usaa\" + 0.006*\"everbank\"\n",
      "\n",
      "\n",
      "The topic: 8 Word: 0.094*\"well\" + 0.093*\"fargo\" + 0.028*\"citimortgag\" + 0.019*\"lieu\" + 0.012*\"converg\" + 0.011*\"depot\" + 0.011*\"navi\" + 0.007*\"maci\" + 0.007*\"vacant\" + 0.007*\"hardest\"\n",
      "\n",
      "\n",
      "The topic: 9 Word: 0.087*\"nationstar\" + 0.031*\"coinbas\" + 0.015*\"equifax\" + 0.013*\"breach\" + 0.011*\"harp\" + 0.009*\"freez\" + 0.008*\"data\" + 0.008*\"secur\" + 0.007*\"bitcoin\" + 0.007*\"inaccuraci\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For each topic, we will explore the words occuring in that topic and its relative weight\n",
    "'''\n",
    "for id_x, topic in LDA_model_tfidf.print_topics(-1):\n",
    "    print(\"The topic: {} Word: {}\".format(id_x, topic))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of the topics ###\n",
    "\n",
    "As we can see, when using tf-idf, heavier weights are given to words that are not as frequent which results in nouns being factored in. That makes it harder to figure out the categories as nouns can be hard to categorize. This goes to show that the models we apply depend on the type of corpus of text we are dealing with. \n",
    "\n",
    "Using the words in each topic and their corresponding weights, what categories could you find?\n",
    "\n",
    "* 0: \n",
    "* 1:  \n",
    "* 2: \n",
    "* 3: \n",
    "* 4:  \n",
    "* 5: \n",
    "* 6: \n",
    "* 7: \n",
    "* 8: \n",
    "* 9: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.1: Performance evaluation by classifying sample document using LDA Bag of Words model##\n",
    "\n",
    "We will check to see where our test document would be classified. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['close',\n",
       " 'account',\n",
       " 'feder',\n",
       " 'lawsuit',\n",
       " 'prepar',\n",
       " 'file',\n",
       " 'send',\n",
       " 'money',\n",
       " 'hold',\n",
       " 'money',\n",
       " 'wont',\n",
       " 'tell',\n",
       " 'send',\n",
       " 'money']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Text of sample document 4310\n",
    "'''\n",
    "processed_data[146]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.41861197352409363\t \n",
      "Topic: 0.110*\"account\" + 0.108*\"bank\" + 0.045*\"check\" + 0.027*\"america\" + 0.026*\"charg\" + 0.025*\"money\" + 0.023*\"close\" + 0.020*\"fund\" + 0.018*\"balanc\" + 0.016*\"pay\"\n",
      "\n",
      "Score: 0.24240322411060333\t \n",
      "Topic: 0.044*\"call\" + 0.040*\"tell\" + 0.026*\"say\" + 0.025*\"send\" + 0.025*\"receiv\" + 0.019*\"ask\" + 0.019*\"phone\" + 0.018*\"time\" + 0.017*\"contact\" + 0.017*\"speak\"\n",
      "\n",
      "Score: 0.2113182544708252\t \n",
      "Topic: 0.034*\"request\" + 0.033*\"letter\" + 0.027*\"document\" + 0.027*\"send\" + 0.025*\"inform\" + 0.024*\"provid\" + 0.016*\"file\" + 0.016*\"account\" + 0.015*\"report\" + 0.015*\"receiv\"\n",
      "\n",
      "Score: 0.08765189349651337\t \n",
      "Topic: 0.052*\"debt\" + 0.033*\"collect\" + 0.015*\"court\" + 0.012*\"compani\" + 0.011*\"attorney\" + 0.011*\"state\" + 0.009*\"agenc\" + 0.009*\"legal\" + 0.008*\"claim\" + 0.007*\"servic\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA Bag of Words model.\n",
    "'''\n",
    "\n",
    "# Our test document is document number 4310\n",
    "for index, score in sorted(LDA_model[BoW_data[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, LDA_model.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It has the highest probability (`x`) to be  part of the topic that we assigned as X, which is the accurate classification. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5.2: Performance evaluation by classifying sample document using LDA TF-IDF model##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.6701599955558777\t \n",
      "Topic: 0.012*\"bank\" + 0.011*\"payment\" + 0.010*\"check\" + 0.009*\"account\" + 0.009*\"money\" + 0.009*\"charg\" + 0.008*\"card\" + 0.007*\"transfer\" + 0.007*\"fund\" + 0.006*\"transact\"\n",
      "\n",
      "Score: 0.2764914035797119\t \n",
      "Topic: 0.011*\"debt\" + 0.007*\"mortgag\" + 0.006*\"collect\" + 0.006*\"foreclosur\" + 0.006*\"properti\" + 0.005*\"document\" + 0.005*\"sale\" + 0.005*\"loan\" + 0.005*\"court\" + 0.005*\"letter\"\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Check which topic our test document belongs to using the LDA TF-IDF model.\n",
    "'''\n",
    "for index, score in sorted(LDA_model_tfidf[BoW_data[document_num]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, LDA_model_tfidf.print_topic(index, 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It has the highest probability (`x%`) to be  part of the topic that we assigned as X. ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing model on unseen document ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 0.4725496172904968\t Topic: 0.056*\"chase\" + 0.027*\"fraud\" + 0.024*\"secur\" + 0.020*\"inform\" + 0.017*\"person\"\n",
      "Score: 0.2120215743780136\t Topic: 0.120*\"payment\" + 0.042*\"month\" + 0.024*\"pay\" + 0.020*\"late\" + 0.016*\"time\"\n",
      "Score: 0.19608375430107117\t Topic: 0.044*\"call\" + 0.040*\"tell\" + 0.026*\"say\" + 0.025*\"send\" + 0.025*\"receiv\"\n",
      "Score: 0.017050854861736298\t Topic: 0.046*\"credit\" + 0.045*\"card\" + 0.026*\"citi\" + 0.022*\"capit\" + 0.016*\"call\"\n",
      "Score: 0.01705033704638481\t Topic: 0.110*\"account\" + 0.108*\"bank\" + 0.045*\"check\" + 0.027*\"america\" + 0.026*\"charg\"\n",
      "Score: 0.017049968242645264\t Topic: 0.052*\"debt\" + 0.033*\"collect\" + 0.015*\"court\" + 0.012*\"compani\" + 0.011*\"attorney\"\n",
      "Score: 0.017049238085746765\t Topic: 0.145*\"credit\" + 0.126*\"report\" + 0.048*\"account\" + 0.032*\"remov\" + 0.018*\"inform\"\n",
      "Score: 0.017048416659235954\t Topic: 0.034*\"request\" + 0.033*\"letter\" + 0.027*\"document\" + 0.027*\"send\" + 0.025*\"inform\"\n",
      "Score: 0.01704840362071991\t Topic: 0.026*\"servic\" + 0.015*\"request\" + 0.014*\"email\" + 0.013*\"receiv\" + 0.013*\"process\"\n",
      "Score: 0.01704780012369156\t Topic: 0.072*\"loan\" + 0.050*\"mortgag\" + 0.023*\"home\" + 0.019*\"modif\" + 0.013*\"properti\"\n"
     ]
    }
   ],
   "source": [
    "unseen_document = \"My favorite sports activities are running and swimming.\"\n",
    "\n",
    "# Data preprocessing step for the unseen document\n",
    "BoW_vector = BoW_dictionary.doc2bow(preprocess(unseen_document))\n",
    "\n",
    "for index, score in sorted(LDA_model[BoW_vector], key=lambda tup: -1*tup[1]):\n",
    "    print(\"Score: {}\\t Topic: {}\".format(score, LDA_model.print_topic(index, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model correctly classifies the unseen document with 'x'% probability to the X category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
